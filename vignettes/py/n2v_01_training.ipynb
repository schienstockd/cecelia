{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise2Void - 3D Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We import all our dependencies.\n",
    "from n2v.models import N2VConfig, N2V\n",
    "import numpy as np\n",
    "from csbdeep.utils import plot_history\n",
    "from n2v.utils.n2v_utils import manipulate_val_data\n",
    "from n2v.internals.N2V_DataGenerator import N2V_DataGenerator\n",
    "from matplotlib import pyplot as plt\n",
    "import urllib\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Example Data\n",
    "Thanks to Romina Piscitello (Eaton Lab, MPI-CBG) for letting us use her data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a folder for our data\n",
    "if not os.path.isdir('./data'):\n",
    "    os.mkdir('./data')\n",
    "\n",
    "# check if data has been downloaded already\n",
    "zipPath='data/flywing-data.zip'\n",
    "if not os.path.exists(zipPath):\n",
    "    #download and unzip data\n",
    "    data = urllib.request.urlretrieve('https://download.fht.org/jug/n2v/flywing-data.zip', zipPath)\n",
    "    with zipfile.ZipFile(zipPath, 'r') as zip_ref:\n",
    "        zip_ref.extractall('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data Preparation\n",
    "For training we will load __one__ low-SNR 3D-tif-volume and use the <code>N2V_DataGenerator</code> to extract non-overlapping 3D-patches. Since N2V is a self-supervised method, we don't need targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We create our DataGenerator-object.\n",
    "# It will help us load data and extract patches for training and validation.\n",
    "datagen = N2V_DataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method generate_patches_from_list in module n2v.internals.N2V_DataGenerator:\n",
      "\n",
      "generate_patches_from_list(data, num_patches_per_img=None, shape=(256, 256), augment=True, shuffle=False) method of n2v.internals.N2V_DataGenerator.N2V_DataGenerator instance\n",
      "    Extracts patches from 'list_data', which is a list of images, and returns them in a 'numpy-array'. The images\n",
      "    can have different dimensionality.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    data                : list(array(float))\n",
      "                          List of images with dimensions 'SZYXC' or 'SYXC'\n",
      "    num_patches_per_img : int, optional(default=None)\n",
      "                          Number of patches to extract per image. If 'None', as many patches as fit i nto the\n",
      "                          dimensions are extracted.\n",
      "    shape               : tuple(int), optional(default=(256, 256))\n",
      "                          Shape of the extracted patches.\n",
      "    augment             : bool, optional(default=True)\n",
      "                          Rotate the patches in XY-Plane and flip them along X-Axis. This only works if the patches are square in XY.\n",
      "    shuffle             : bool, optional(default=False)\n",
      "                          Shuffles extracted patches across all given images (data).\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    patches : array(float)\n",
      "              Numpy-Array with the patches. The dimensions are 'SZYXC' or 'SYXC'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(datagen.generate_patches_from_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will load all the '.tif' files from the 'data' directory. In our case it is only one.\n",
    "# The function will return a list of images (numpy arrays).\n",
    "# In the 'dims' parameter we specify the order of dimensions in the image files we are reading.\n",
    "imgs = datagen.load_imgs_from_directory(directory = \"data/\", dims='ZYX')\n",
    "\n",
    "# Let's look at the shape of the image\n",
    "print(imgs[0].shape)\n",
    "# The function automatically added two extra dimension to the images:\n",
    "# One at the front is used to hold a potential stack of images such as a movie.\n",
    "# One at the end could hold color channels such as RGB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at a maximum projection of the volume.\n",
    "# We have to remove the added extra dimensions to display it.\n",
    "plt.figure(figsize=(32,16))\n",
    "plt.imshow(np.max(imgs[0][0,...,0],axis=0), \n",
    "           cmap='magma',\n",
    "           vmin=np.percentile(imgs[0],0.1),\n",
    "           vmax=np.percentile(imgs[0],99.9)\n",
    "          )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we extract patches for training and validation.\n",
    "patch_shape = (32, 64, 64)\n",
    "patches = datagen.generate_patches_from_list(imgs[:1], shape=patch_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patches are created so they do not overlap.\n",
    "# (Note: this is not the case if you specify a number of patches. See the docstring for details!)\n",
    "# Non-overlapping patches enable us to split them into a training and validation set.\n",
    "X = patches[:600]\n",
    "X_val = patches[600:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just in case you don't know how to access the docstring of a method:\n",
    "datagen.generate_patches_from_list?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at two patches.\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(X[0,16,...,0],cmap='magma')\n",
    "plt.title('Training Patch');\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(X_val[0,16,...,0],cmap='magma')\n",
    "plt.title('Validation Patch');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure\n",
    "Noise2Void comes with a special config-object, where we store network-architecture and training specific parameters. See the docstring of the <code>N2VConfig</code> constructor for a description of all parameters.\n",
    "\n",
    "When creating the config-object, we provide the training data <code>X</code>. From <code>X</code> we extract <code>mean</code> and <code>std</code> that will be used to normalize all data before it is processed by the network. We also extract the dimensionality and number of channels from <code>X</code>.\n",
    "\n",
    "Compared to supervised training (i.e. traditional CARE), we recommend to use N2V with an increased <code>train_batch_size</code> and <code>batch_norm</code>.\n",
    "To keep the network from learning the identity we have to manipulate the input pixels during training. For this we have the parameter <code>n2v_manipulator</code> with default value <code>'uniform_withCP'</code>. Most pixel manipulators will compute the replacement value based on a neighborhood. With <code>n2v_neighborhood_radius</code> we can control its size. \n",
    "\n",
    "Other pixel manipulators:\n",
    "* normal_withoutCP: samples the neighborhood according to a normal gaussian distribution, but without the center pixel\n",
    "* normal_additive: adds a random number to the original pixel value. The random number is sampled from a gaussian distribution with zero-mean and sigma = <code>n2v_neighborhood_radius</code>\n",
    "* normal_fitted: uses a random value from a gaussian normal distribution with mean equal to the mean of the neighborhood and standard deviation equal to the standard deviation of the neighborhood.\n",
    "* identity: performs no pixel manipulation\n",
    "\n",
    "For faster training multiple pixels per input patch can be manipulated. In our experiments we manipulated about 0.198% of the input pixels per patch. For a patch size of 64 by 64 pixels this corresponds to about 8 pixels. This fraction can be tuned via <code>n2v_perc_pix</code>.\n",
    "\n",
    "For Noise2Void training it is possible to pass arbitrarily large patches to the training method. From these patches random subpatches of size <code>n2v_patch_shape</code> are extracted during training. Default patch shape is set to (64, 64), but since this is an 3D example we obviously need to specify a triple, here (32, 64, 64).  \n",
    "\n",
    "In the past we experienced bleedthrough artifacts between channels if training was terminated to early. To counter bleedthrough we added the `single_net_per_channel` option, which is turned on by default. In the back a single U-Net for each channel is created and trained independently, thereby removing the possiblity of bleedthrough. <br/>\n",
    "__Note:__ Essentially the network gets multiplied by the number of channels, which increases the memory requirements. If your GPU gets too small, you can always split the channels manually and train a network for each channel one after another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Warning:</font> to make this example notebook execute faster, we have set <code>train_epochs</code> to only 20. <br>For better results we suggest 100 to 200 <code>train_epochs</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can increase \"train_steps_per_epoch\" to get even better results at the price of longer computation. \n",
    "config = N2VConfig(X, unet_kern_size=3, \n",
    "                   train_steps_per_epoch=int(X.shape[0]/128),train_epochs=20, train_loss='mse', batch_norm=True, \n",
    "                   train_batch_size=4, n2v_perc_pix=0.198, n2v_patch_shape=(32, 64, 64), \n",
    "                   n2v_manipulator='uniform_withCP', n2v_neighborhood_radius=5)\n",
    "\n",
    "# Let's look at the parameters stored in the config-object.\n",
    "vars(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a name used to identify the model\n",
    "model_name = 'n2v_3D'\n",
    "# the base directory in which our model will live\n",
    "basedir = 'models'\n",
    "# We are now creating our network model.\n",
    "model = N2V(config=config, name=model_name, basedir=basedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.train(X, X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After training, lets plot training and validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(list(history.history.keys())))\n",
    "plt.figure(figsize=(16,5))\n",
    "plot_history(history,['loss','val_loss']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Model in BioImage ModelZoo Format\n",
    "See https://imagej.net/N2V#Prediction for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.export_TF(name='Noise2Void - 3D Example', \n",
    "                description='This is the 3D Noise2Void example trained in python.', \n",
    "                authors=[\"Tim-Oliver Buchholz\", \"Alexander Krull\", \"Florian Jug\"],\n",
    "                test_img=X_val[0,...], axes='ZYXC',\n",
    "                patch_shape=patch_shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
