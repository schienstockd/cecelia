{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise2Void - 2D Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "# import working directory to check functions\n",
    "os.chdir('/Users/Dominik/R-workspace/cecelia/inst')\n",
    "\n",
    "# MacOS\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "# config\n",
    "import py.config_utils as cfg\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import all our dependencies.\n",
    "from n2v.models import N2VConfig, N2V\n",
    "import numpy as np\n",
    "from csbdeep.utils import plot_history\n",
    "from n2v.utils.n2v_utils import manipulate_val_data\n",
    "from n2v.internals.N2V_DataGenerator import N2V_DataGenerator\n",
    "from matplotlib import pyplot as plt\n",
    "import urllib\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create our DataGenerator-object.\n",
    "# It will help us load data and extract patches for training and validation.\n",
    "datagen = N2V_DataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/Volumes/USER_data/Dominik/CECELIA_BACKUP/CV5iNI/ANALYSIS/'\n",
    "version_num = 1\n",
    "task_dir = os.path.join(base_dir, str(version_num), 'fmjlDg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tom\n",
    "uids = ['ocGpqr', 'FSotUw', 'vJV9by', 'tM949V', '50ymIS', '2DWW5C', '6J6z8t', 'nuL8W0', '5EZW5E', 'WzDKfW', 'ZLATW0', 'Qphi2J', 'MWQBVD', 'YBq5rm', '588Dj0', 'IzjXlv', 'QdjG8p', 'CfTwJD', 'bqdS4D', 'eXhH0u', 'qcwblF', 'I32uRk', '5OfZTY', 'yJyPDn', 'btZFiO', 'zRyuzO', 'EbYjEw', 'eAX4fZ', 'RpFBeG', '0wlkTG', 'TVf7C7', '0WkJ2n', 'pR8OoS', 'D3rG8h', 'wVL31E', 'kI45Yr', 'EFSIfI', 'KuY1Nm', 'ryKCBh', 'IcNmah', 'qvhTEJ', '8yr0h5', '1TqO7r', 'xIkEjm', 'gf6vXf', 'v1De25', 'x8sHW6', 'HXO9hG', 'SF82TQ', '0OvjSI', 'KQZxwr', 'm2n2Ft', 'PGCD8S', 'AdYO7L', 'SYXMth', '7dBggN', 'z6Jti0', 'XnVCIm', 'vjENx0', 'WFpe38', 'OWodvg', '0nNQWs', 'QrRXBk', 'GSUOLg', 'qC8o1T', '27p88D', 'G49ZZx', '23BeFx', 'g0jRwo', 'E14r6I']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XCR1\n",
    "uids = ['WvmInm', '43Wrm6', 'zGJ6Zx', 'r0lLnq', 'irQcyT', 'xrpULx', 'NJvsNY', 'RgC7Wn', 'F5JvHp', '0XIgdc', 'VsY1bp', '5uVnh2', '3nNcPG', 'AZ1f7k', 'FcbjWa', '7oh2II', 'xAivFa', 'DyLaDq', 'ZAWNaT', 'HhKbUR', 'COnsaW', 'HTWRuh', '2JxLGc', 'YuMg8j', 'pnjMea', 'r30OyS', 'mskvXN', 'FrimIN', 'stm1J4', '4Ky14J', 'Ti2xhq', 'ajiVG3', 'z43ZmK', 'a5N98R', 'EWyt7S', 'uxXWQW', 'Rj5rxu', 'zsROWp', 'NSupPO', 'encBlg', 'KDVAxS', 'H9uFRN', 'lFuBwE', '9GaL76', 'ApHcZ5', 'jiI7v6', 'kStCa2', '8M6efc', 'Tvr1BS', '7SFBI6', 'PuViWT', 'TqPA9L', '5nTQdi', 'EMaQj2', 'DSRdqh', 'yz1yhZ', '0niKqI', 'rDKeGd', 'uhf9Xx', '5mwG5V', '8uHyK7', 'zBZm20', 'JhdPZU', 'RufKxs', 'FHIGav', 'vkAjt0', 'otPUph', 'tZpWza', 'LRttqF', '7LFxXL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import py.zarr_utils as zarr_utils\n",
    "import skimage.filters\n",
    "\n",
    "# get zarr\n",
    "# imgs = {i: zarr_utils.open_as_zarr(os.path.join(base_dir, '0', i, 'ccidImage.zarr')) for i in uids}\n",
    "imgs = [zarr_utils.open_as_zarr(os.path.join(base_dir, '0', i, 'ccidImage.zarr')) for i in uids]\n",
    "\n",
    "# get arrays\n",
    "# imgs = {i: np.squeeze(np.array(zarr_utils.fortify(x[0]))) for i, x in imgs.items()}\n",
    "imgs = [np.squeeze(np.array(zarr_utils.fortify(x[0]))) for x in imgs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add gaussian\n",
    "# imgs = [skimage.filters.gaussian(x, sigma = 1) for x in imgs]\n",
    "\n",
    "# add time\n",
    "imgs = [np.expand_dims(x, axis = 0) for x in imgs]\n",
    "\n",
    "# add RGB\n",
    "imgs = [np.expand_dims(x, axis = len(imgs[0].shape)) for x in imgs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(len(imgs), imgs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we extract patches for training and validation.\n",
    "# Try 3D?\n",
    "patch_shape = (4, 64, 64)\n",
    "#patch_shape = (64, 128, 128)\n",
    "\n",
    "patches = datagen.generate_patches_from_list(imgs[:1], shape=patch_shape, shuffle=True)\n",
    "\n",
    "# Patches are created so they do not overlap.\n",
    "# (Note: this is not the case if you specify a number of patches. See the docstring for details!)\n",
    "# Non-overlapping patches enable us to split them into a training and validation set.\n",
    "split_val = round(len(patches) * 0.8)\n",
    "\n",
    "X = patches[:split_val]\n",
    "X_val = patches[split_val:]\n",
    "\n",
    "train_steps_per_epoch = int(X.shape[0]/64)\n",
    "#train_steps_per_epoch = int(X.shape[0]/16)\n",
    "\n",
    "#train_epochs = 100\n",
    "train_epochs = 20\n",
    "\n",
    "(len(X), len(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at two patches.\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(np.max(X[0,...,0], axis = 0),cmap='magma')\n",
    "plt.title('Training Patch');\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(np.max(X_val[0,...,0], axis = 0),cmap='magma')\n",
    "plt.title('Validation Patch');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure\n",
    "Noise2Void comes with a special config-object, where we store network-architecture and training specific parameters. See the docstring of the <code>N2VConfig</code> constructor for a description of all parameters.\n",
    "\n",
    "When creating the config-object, we provide the training data <code>X</code>. From <code>X</code> we extract <code>mean</code> and <code>std</code> that will be used to normalize all data before it is processed by the network. We also extract the dimensionality and number of channels from <code>X</code>.\n",
    "\n",
    "Compared to supervised training (i.e. traditional CARE), we recommend to use N2V with an increased <code>train_batch_size</code> and <code>batch_norm</code>.\n",
    "To keep the network from learning the identity we have to manipulate the input pixels during training. For this we have the parameter <code>n2v_manipulator</code> with default value <code>'uniform_withCP'</code>. Most pixel manipulators will compute the replacement value based on a neighborhood. With <code>n2v_neighborhood_radius</code> we can control its size. \n",
    "\n",
    "Other pixel manipulators:\n",
    "* normal_withoutCP: samples the neighborhood according to a normal gaussian distribution, but without the center pixel\n",
    "* normal_additive: adds a random number to the original pixel value. The random number is sampled from a gaussian distribution with zero-mean and sigma = <code>n2v_neighborhood_radius</code>\n",
    "* normal_fitted: uses a random value from a gaussian normal distribution with mean equal to the mean of the neighborhood and standard deviation equal to the standard deviation of the neighborhood.\n",
    "* identity: performs no pixel manipulation\n",
    "\n",
    "For faster training multiple pixels per input patch can be manipulated. In our experiments we manipulated about 0.198% of the input pixels per patch. For a patch size of 64 by 64 pixels this corresponds to about 8 pixels. This fraction can be tuned via <code>n2v_perc_pix</code>.\n",
    "\n",
    "For Noise2Void training it is possible to pass arbitrarily large patches to the training method. From these patches random subpatches of size <code>n2v_patch_shape</code> are extracted during training. Default patch shape is set to (64, 64), but since this is an 3D example we obviously need to specify a triple, here (32, 64, 64).  \n",
    "\n",
    "In the past we experienced bleedthrough artifacts between channels if training was terminated to early. To counter bleedthrough we added the `single_net_per_channel` option, which is turned on by default. In the back a single U-Net for each channel is created and trained independently, thereby removing the possiblity of bleedthrough. <br/>\n",
    "__Note:__ Essentially the network gets multiplied by the number of channels, which increases the memory requirements. If your GPU gets too small, you can always split the channels manually and train a network for each channel one after another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Warning:</font> to make this example notebook execute faster, we have set <code>train_epochs</code> to only 20. <br>For better results we suggest 100 to 200 <code>train_epochs</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can increase \"train_steps_per_epoch\" to get even better results at the price of longer computation. \n",
    "config = N2VConfig(X, unet_kern_size=3, \n",
    "                   train_steps_per_epoch=train_steps_per_epoch,train_epochs=train_epochs, train_loss='mse', batch_norm=True, \n",
    "                   train_batch_size=4, n2v_perc_pix=0.198, n2v_patch_shape=patch_shape, \n",
    "                   n2v_manipulator='uniform_withCP', n2v_neighborhood_radius=5)\n",
    "\n",
    "# Let's look at the parameters stored in the config-object.\n",
    "vars(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a name used to identify the model\n",
    "model_name = 'n2v_3P_ubiTom'\n",
    "\n",
    "# the base directory in which our model will live\n",
    "basedir = '/Users/Dominik/Downloads/n2v'\n",
    "\n",
    "# We are now creating our network model.\n",
    "model = N2V(config=config, name=model_name, basedir=basedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.train(X, X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After training, lets plot training and validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(list(history.history.keys())))\n",
    "plt.figure(figsize=(16,5))\n",
    "plot_history(history,['loss','val_loss']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Model in BioImage ModelZoo Format\n",
    "See https://imagej.net/N2V#Prediction for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.export_TF(\n",
    "    name='Noise2Void - 3P ubiTom', \n",
    "    description='!TEST! 3P ubiTom from 3D stack', \n",
    "    authors=[\"Dominik Schienstock\"],\n",
    "    test_img=X_val[0,...], axes='ZYXC',\n",
    "    patch_shape=patch_shape,\n",
    "    result_path=basedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
